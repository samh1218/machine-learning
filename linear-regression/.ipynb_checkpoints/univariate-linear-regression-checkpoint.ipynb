{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some crucial libraries for building our Univariate Linear Regression Model\n",
    "import numpy as np                # for some crucial (linear algebra) computation\n",
    "import pandas as pd               # for data structures and data analysis\n",
    "import matplotlib.pyplot as plt  # for plotting our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our data, we are going to use our Kaggle Dataset :\n",
    "data = pd.read_csv('restaurant-univariate.csv')\n",
    "# For now, we will use 'pandas' to do the reading of data for us.\n",
    "# In the future, we will have a coding exercise where you should be familiar creating\n",
    "# a csv reader and writer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Grade School, we learned that a Univariate Linear Regression is written as :\n",
    "\n",
    "$y = mx + b$ where :\n",
    "1. $y = $ the output\n",
    "2. $m$ and $b$ are 'weights'\n",
    "3. $x$ is the feature\n",
    "\n",
    "#### In Machine Learning, we write the same function as :\n",
    "\n",
    "$h(x)=\\theta_1 * x_1 + \\theta_2 * x_2$ where :\n",
    "1. $\\theta_1$ and $\\theta_2$ are 'weights'\n",
    "2. $x_1$ is the default feature and equal to $1$\n",
    "3. $x_2$ is the single feature for the univariate linear regression.\n",
    "\n",
    "#### We can re-write this same function into a vectorized format\n",
    "$h_\\theta(x) = \\theta_1 * x_1 + \\theta_2 * x_2$\n",
    "\n",
    "$h_\\theta(X) = \\theta^{T}X$ where :\n",
    "1. $\\theta^{T}$ is a transpose vector : $\\begin{bmatrix}\\theta_1 & \\theta_2 \\end{bmatrix}$\n",
    "2. $X$ is a vector for features : $\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions :\n",
    "\n",
    "$Q_1$ : During the training phase, which of the following are we trying to tweak?\n",
    "\n",
    "a.) the weights, b.) the features, c.) the output\n",
    "\n",
    "$Q_2$ : What is the difference between 'dimensions' and 'features'? (This is a trick question)\n",
    "\n",
    "$Q_3$ : What is 'Regression'?\n",
    "\n",
    "$Q_4$ : In a regression line, with each increase or decrease of the standard deviation in feature x, how much is the output $y$ increased? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should retrieve the values of the 'head' so you can retrieve the value.\n",
    "# --> INSERT BELOW\n",
    "\n",
    "# Let's first collect the values for x_1 and x_2 and y.\n",
    "# Make sure to specify the keys of the data!\n",
    "# By default, all the weights in Theta should be 0!\n",
    "# UNCOMMENT WHEN READY!\n",
    "# X = \n",
    "# Y = \n",
    "# Theta = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest for the General/Multivariate Linear Regression Function\n",
    "\n",
    "Now, try to implement a linear regression function when we have 2 features. After that, try doing it for one that is more general a.k.a. works for 2 or more features.\n",
    "\n",
    "When you are ready, read on.\n",
    "\n",
    ">The General Multivariate Linear Regression : $h_\\theta(X) = \\theta^{T}X$ where :\n",
    "1. $\\theta^{T}$ is a transpose vector : $\\begin{bmatrix}\\theta_1 & \\theta_2 & \\theta_3 & ... & \\theta_n \\end{bmatrix}$ where n is the number of features\n",
    "2. $X$ is a vector for features : $\\begin{bmatrix}x_1 \\\\ x_2 \\\\x_3 \\\\ ... \\\\x_n \\end{bmatrix}$ where n is the number of features.\n",
    "\n",
    "#### Wait...what?? Why does the general linear regression function look the same??\n",
    "Well yes, it's the same function, if you think about it. All we really need to do is just add more features and weights into both $\\theta$ and $X$.\n",
    "\n",
    "In other words, this general linear regression function is simply a collapsed form of the linear regression function you have learned in grade school.\n",
    "\n",
    "Really, it's just the power of Linear Algebra..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE, implement the hypothesis we have come up with\n",
    "# h = Theta * X\n",
    "# 1. Make sure 'Theta' is a transpose vector and X is a regular vector\n",
    "# 2. Make sure you round down the output to the second decimal position\n",
    "# example  : y = 1.27898 -> y = 1.27\n",
    "# @param Theta : the weights. This is a numpy array (not yet transposed)\n",
    "# @param X : the features. This is a numpy array\n",
    "# @return prediction of price, in float\n",
    "#-------------------------\n",
    "# UNCOMMENT WHEN READY\n",
    "#-------------------------\n",
    "def hypothesis(Theta, X):\n",
    "    tTheta = np.transpose(Theta)\n",
    "    result = tTheta * X\n",
    "    return round(np.sum(result),2)\n",
    "\n",
    "# TESTS (NOt used for grading)---------------\n",
    "tTheta = np.array([0.5, 1.7, -2.1])\n",
    "tX = np.array([1, 121.3, 14.57])\n",
    "assert(hypothesis(tTheta, tX) == 176.11)\n",
    "tTheta = np.array([-321.3, 4.3, 7.9])\n",
    "tX = np.array([1, -49.89, -15.84])\n",
    "assert(hypothesis(tTheta, tX) == -660.96)\n",
    "tTheta = np.array([213, 23, 3])\n",
    "tX = np.array([1, -23, 51])\n",
    "assert(hypothesis(tTheta, tX) == -163.0)\n",
    "# Write more of your tests to make sure the hypothesis works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training : The Quest for the Ideal 'Weights'\n",
    "As mentioned in the past, the goal of 'training' in machine learning is to find the values for the weights of the function so that the function with those weights and features can predict an outcome it has never seen quite accurately.\n",
    "\n",
    "In Training, we use the 'cost function' to measure how well the predictor line fits to the data we are seeing (known as 'observations' or the 'training set examples').\n",
    "\n",
    "### The Cost Function\n",
    "Essentially, the cost of the regression line is the vertical distance between each observation (e.g. training example) to the regression line.\n",
    "\n",
    "The regression line with the least amount of error is the one we call the 'best fit' line or the final regression line at the end of training.\n",
    "\n",
    "#### So What is the Cost Function?\n",
    "As we said, the distance between each point to the line is itself the 'cost' of the regression line.\n",
    "\n",
    "So, we need :\n",
    "\n",
    "- Summation to compute the total distance by adding up every point's distance to the regression line\n",
    "- For each value in the summation, an equation to compute the 'distance'\n",
    "\n",
    "Let's see the function for a univariate linear regression line.\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = 1 \\div 2m \\sum_{i=1}^{M}(h_{\\theta_0,\\theta_1}(x_i) - y_i)^{2} $\n",
    "\n",
    "We keep the '2' to help us later with the partial derivative.\n",
    "\n",
    "Now, try it on your own to construct a general cost function of multivariate linear regression line. Use your knowledge of Linear Algebra to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Cost Function Here.\n",
    "# Your Cost Function will take a numpy array of the weights 'Theta',\n",
    "# a numpy array of features, and a numpy array of actual outputs associated with the features\n",
    "# and output a scalar value, in float.\n",
    "#\n",
    "# NOTE : the size of Y will give you the # of training examples in your training set\n",
    "#\n",
    "# WHEN YOU ARE READY, UNCOMMENT THE FUNCTION BELOW.\n",
    "# def cost(Theta, X, Y):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Glossary of Terms\n",
    "1. $x_j^{i}$ : Single feature vector for training example i\n",
    "    1. i : index of training example\n",
    "    2. j : index of the feature in the feature vector.\n",
    "2. Capitalized Letter for variable name = Matrix\n",
    "3. Lowercase Letter for variable name = Vector or Scalar\n",
    "3. 'scalar' is a single-value output. Vector is a single-column matrix.\n",
    "    1. scalar example : y = 1\n",
    "    2. vector example : $x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ ... \\\\ x_n \\end{bmatrix}$\n",
    "    3. matrix example : $K = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$\n",
    "\n",
    "## What is Regression?\n",
    "\n",
    "'Regression' is a study of correlations between two variables. What you are essentially doing in a regression task is to find the nature of the relationship between two things, and if somehow the two are correlated. \n",
    "\n",
    "So, what is 'linear regression?' We know from grade school that the goal of linear regression is to come up with the 'best fit line.'\n",
    "\n",
    "#### Review Questions : \n",
    "Q1 : What does the 'best fit line' represent?\n",
    "Q2 : How do we compute the 'best fit line?'\n",
    "Q3 : What is the point of 'linear regression'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Univariate Linear Regression\n",
    "\n",
    "In Grade School, we learned that a Univariate Linear Regression is written as :\n",
    "\n",
    "$y = mx + b$ where :\n",
    "1. $y = $ the output\n",
    "2. $m$ and $b$ are 'weights'\n",
    "3. $x$ is the feature\n",
    "\n",
    "In essence, we are trying to predict the output by manipulating the values of 'm' and 'x.'\n",
    "\n",
    "While this function is accurate, it's not very useful for us when we need to include more weights and features to do better prediction of y.\n",
    "\n",
    "#### In Machine Learning, we write the same function as :\n",
    "\n",
    "$h(x)=\\theta_1 * x_1 + \\theta_2 * x_2$ where :\n",
    "1. $\\theta_1$ and $\\theta_2$ are 'weights'\n",
    "2. $x_1$ is the default feature and equal to $1$\n",
    "3. $x_2$ is the single feature for the univariate linear regression.\n",
    "\n",
    "Although the same function, we like to keep the weights and features to have the same variable name so that we can more easily identify what is what.\n",
    "\n",
    "#### We can re-write this same function into a vectorized format\n",
    "$h_\\theta(x) = \\theta_1 * x_1 + \\theta_2 * x_2$\n",
    "\n",
    "$h_\\theta(X) = \\theta^{T}x$ where :\n",
    "1. $\\theta^{T}$ is a transpose vector : $\\begin{bmatrix}\\theta_1 & \\theta_2 \\end{bmatrix}$\n",
    "2. $x$ is a vector for features : $\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions :\n",
    "\n",
    "$Q_1$ : During the training phase, which of the following are we trying to tweak?\n",
    "\n",
    "a.) the weights, b.) the features, c.) the output\n",
    "\n",
    "$Q_2$ : What is the difference between 'dimensions' and 'features'? (This is a trick question)\n",
    "\n",
    "$Q_3$ : What is 'Regression'?\n",
    "\n",
    "$Q_4$ : In a regression line, with each increase or decrease of the standard deviation in feature x, how much is the output $y$ increased? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some crucial libraries for building our Univariate Linear Regression Model\n",
    "import numpy as np                # for some crucial (linear algebra) computation\n",
    "import pandas as pd               # for data structures and data analysis\n",
    "import matplotlib.pyplot as plt  # for plotting our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN READY!\n",
    "# Here, you should implement the hypothesis of a SINGLE TRAINING EXAMPLE.\n",
    "#\n",
    "#@param Theta : a vector of weights (numpy)\n",
    "#@param x : a vector of features for ith training example (numpy)\n",
    "#@return a scalar, floating-point value, rounded to the second decimal point.\n",
    "#        This represents the predicted price of 'avg cost for two' for this training example.\n",
    "#\n",
    "#def h(Theta, x):\n",
    "#\n",
    "#\n",
    "#\n",
    "#TESTS (NOT USED FOR GRADING)----------\n",
    "#x1 = np.array([1, 121.0275, 14.56544])\n",
    "#Theta = np.array([25, 0.5, 0.25])\n",
    "#y1 = 89.16\n",
    "#assert(h(Theta,x1) == y1)\n",
    "#x2 = np.array([1, 121.0141, 14.55371])\n",
    "#y2 = 89.15\n",
    "#assert(h(Theta,x2) == y2)\n",
    "#x3 = np.array([1, 121.057, 14.23768])\n",
    "#y3 = 89.09\n",
    "#assert(h(Theta,x3) == y3)\n",
    "#x4 = np.array([1, -47.8818, -15.7641])\n",
    "#y4 = -2.88\n",
    "#assert(h(Theta,x4) == y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest for the General/Multivariate Linear Regression Function for a Single Training Example\n",
    "\n",
    "Now, let's to implement a linear regression function when we have 2 features, excluding the default feature of 1.\n",
    "\n",
    "## Hypothesis for a Single Training EXAMPLE :\n",
    "A vectorized linear regression is :\n",
    "\n",
    "$h_\\theta(x) = \\theta^{T}x$ where $x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ ... \\\\ x_n \\end{bmatrix}$ and $\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ ... \\\\ \\theta_n \\end{bmatrix}$ and 'n' is the number of features.\n",
    "\n",
    "As you can see, all you really need to do is add in more features in the feature vector and an equal number of weights in the weight vector. The function itself does not change.\n",
    "\n",
    "So what does this mean? This exact function you see above is what you will use for all forms of linear regression.\n",
    "\n",
    "NOTE : The output of the above function is for ONE expected value. In other words, the above function is only good for ONE TRAINING EXAMPLE (NOT training set).\n",
    "\n",
    "## Hypothesis for a Training Set\n",
    "A training set is made up of training examples. Each training set is made up of :\n",
    "1. a set of x's (vector)\n",
    "2. and a single output 'y' (scalar)\n",
    "\n",
    "As mentioned, the above equation solves for a single training example.\n",
    "\n",
    "What we want is a hypothesis that can solve an entire training set.\n",
    "\n",
    "### Task : Vectorized Multivariate Linear Regression\n",
    "\n",
    "Given a training set, write a general multivariate linear regression hypothesis. Your general multivariate linear regression should receive all the training examples and weights, and output a vector of scalar values, each pertaining to a training example.\n",
    "\n",
    "Remember, each training set is made up of each training example.\n",
    "\n",
    "DO NOT USE :\n",
    "1. For Loops or Recursions\n",
    "2. Single Training Example Hypothesis Function\n",
    "\n",
    "Each Training example is made up of :\n",
    "1. $x^{i}$ : a vector of features for the ith training example\n",
    "2. $\\theta$ : a vector of weights\n",
    "3. $y^{i}$ : a scalar output.\n",
    "\n",
    "Pro Tips :\n",
    "1. Use Matrix for the features to store the vectors of features for every training example.\n",
    "2. Your 'Y' should be a vector of outputs.\n",
    "3. The vector of weights should remain as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param Theta : the weights. This is a numpy array\n",
    "# @param X : the features. This is a numpy matrix.\n",
    "# @return a vector of scalar values, each position corresponding to \n",
    "# the index of the training example.\n",
    "#-------------------------\n",
    "# UNCOMMENT WHEN READY\n",
    "#-------------------------\n",
    "#def genH(Theta, X):\n",
    "\n",
    "# TESTS (Not used for grading)---------------\n",
    "# Construct your variables down below\n",
    "#gX = np.matrix('1 121.0275 14.56544; 1 121.0141 14.55371; 1 -47.8818 -15.7641')\n",
    "#gTheta = np.array([25, 0.5, 0.25])\n",
    "#gY = np.array([89.16, 89.15, -2.88])\n",
    "#assert(np.array_equal(genH(gTheta, gX), gY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training : The Quest for the Ideal 'Weights'\n",
    "As mentioned in the past, the goal of 'training' in machine learning is to find the values for the weights of the function so that the function with those weights and features can predict an outcome it has never seen quite accurately.\n",
    "\n",
    "In Training, we use the 'cost function' to measure how well the predictor line fits to the data we are seeing (known as 'observations' or the 'training set examples').\n",
    "\n",
    "### The Cost Function\n",
    "Essentially, the cost of the regression line is the vertical distance between each observation (e.g. training example) to the regression line.\n",
    "\n",
    "The regression line with the least amount of error is the one we call the 'best fit' line or the final regression line at the end of training.\n",
    "\n",
    "#### So What is the Cost Function?\n",
    "As we said, the distance between each point to the line is itself the 'cost' of the regression line.\n",
    "\n",
    "So, we need :\n",
    "\n",
    "- Summation to compute the total distance by adding up every point's distance to the regression line\n",
    "- For each value in the summation, an equation to compute the 'distance'\n",
    "\n",
    "Let's see the function for a univariate linear regression line.\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = (\\sum_{i=1}^{M}(h_{\\theta_0,\\theta_1}(x^{i}) - y^{i})^{2}) \\div (2m) $\n",
    "\n",
    "NOTE :\n",
    "1. The exponent for 'X' and 'y' are indices for identifying in the training set. A training set is composed of training examples and each training example has a set of X's associated to a scalar 'y.'\n",
    "2. In this example, $x^{i}$ is a vector and $y^{i}$ is a scalar.\n",
    "\n",
    "Let's now try to convert this so we have vector inputs and not scalar inputs.\n",
    "\n",
    "We keep the '2' to help us later with the partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Cost Function\n",
    "\n",
    "Now, try to come up with first the regular cost function, then the vectorized cost function.\n",
    "\n",
    "The regular cost function should look similar to the above cost function.\n",
    "\n",
    "Your parameters should be :\n",
    "1. $\\theta$ : Vector of Weights. These are always the same for every training example.\n",
    "2. $x^{i}$ : vector of features for the ith training example\n",
    "3. $y$ : vector for expected outputs\n",
    "\n",
    "The cost function should return the total cost of the line.\n",
    "\n",
    "USE :\n",
    "1. the cost_one_example function inside the 'cost' function.\n",
    "2. For loops to iterate through both the 'listX' and 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Regular Cost Function Here.\n",
    "#\n",
    "# NOTE : the size of Y will give you the # of training examples in your training set\n",
    "#\n",
    "#@param Theta the array of weights\n",
    "#@param x the array of features for ith training example\n",
    "#@param y the expected output for ith training example\n",
    "#@return expected cost for one training example.\n",
    "#def cost_one_example(Theta, x, y):\n",
    "#\n",
    "#\n",
    "#\n",
    "#@param Theta the array of weights\n",
    "#@param listX : list of vectors of X (a matrix)\n",
    "#@param Y \n",
    "#def cost(Theta, listX, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Cost Function\n",
    "Now, let's try to come up with the vectorized cost function. Like the vectorized hypothesis, the vectorized cost function should utilize the following parameters :\n",
    "\n",
    "1. $\\theta$ : vector of weights\n",
    "2. $X$ : matrix of weights. Each row represents a single training example's features.\n",
    "3. $y$ : vector of outputs.\n",
    "\n",
    "The vectorized cost function SHOULD NOT use any for loops. DO NOT use the cost_one_example function above to implement the vectorized cost function. Rely only on matrix multiplication, addition, subtraction, and transposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the vectorized cost function here : \n",
    "# UNCOMMENT WHEN READY!\n",
    "#\n",
    "#USE NUMPY.\n",
    "#x is the matrix of features, where each row refers to the same index value in y. Ex. x[0] -> y[0]\n",
    "#y is the vector of outputs. Each output index refers to the row of features in x. Ex. y[0] -> x[0]\n",
    "#theta is the vector of weights.\n",
    "#def vCost(Theta, x, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Gradient Descent\n",
    "By 'training', we are trying to find the ideal value for the weights, given the feature values and expected outputs we have seen, to create the minimum average vertical distance between every single training example to the regression line. We call this the 'best fit line.'\n",
    "\n",
    "> The 'best fit line' a regression line with the minimum average vertical distance between every point and the line.\n",
    "\n",
    "What we are essentially trying to do is the change the weights of the regression line just enough so that we achieve the lowest possible cost for the regression line, given what we have seen (observations -> the outputs).\n",
    "\n",
    "The algorithm we use to achieve this regression line is Gradient Descent.\n",
    "\n",
    "### The Algorithm (Courtesy by Andrew Ng's Cource : https://www.coursera.org/learn/machine-learning/supplement/aEN5G/gradient-descent-for-multiple-variables)\n",
    "Officially, we want to change every weight in the regression line simultaneously so that the tangent to the cost function is 0 (essentially, we want to take the derivative of it).\n",
    "\n",
    "#### Repeat until convergence and Compute Simultaneously : {\n",
    "1. $\\theta_0=\\theta_0 - \\alpha (1/m) \\sum_{i=1}^{m}(h_\\theta(x^{i}) - y^{i})*x_0^{i}$\n",
    "2. $\\theta_1=\\theta_1 - \\alpha (1/m) \\sum_{i=1}^{m}(h_\\theta(x^{i}) - y^{i})*x_1^{i}$\n",
    "3. $\\theta_2=\\theta_2 - \\alpha (1/m) \\sum_{i=1}^{m}(h_\\theta(x^{i}) - y^{i})*x_2^{i}$\n",
    "4. ...\n",
    "\n",
    "}\n",
    "\n",
    "Essentially :\n",
    "#### Repeat until convergence and Compute Simultaneously :\n",
    "$\\theta_j=\\theta_j - \\alpha (1/m) \\sum_{i=1}^{m}(h_\\theta(x^{i}) - y^{i})*x_j^{i}$\n",
    "\n",
    "1. m = the total number of training examples\n",
    "2. $\\alpha$ = the learning rate.\n",
    "\n",
    "The $\\alpha$ is the 'learning rate.' This variable determines how quickly we attempt to reach down to the lowest point. Too big an alpha and we would actually 'skip' the lowest point, causing us more time to train and even becoming an endless loop; too small an alpha rate and it causes us more time to train.\n",
    "\n",
    "For more information : https://www.coursera.org/learn/machine-learning/supplement/TnHvV/gradient-descent-in-practice-ii-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the NON VECTORIZED Gradient Descent here.\n",
    "# UNCOMMENT WHEN READY!\n",
    "# X is the matrix of features. Each index of x corresponds to the element in y. Ex. x[0][] -> y[0]\n",
    "# y is the vector of outputs. Each index of y corresponds to the row in x. Ex. y[0] -> x[0][]\n",
    "# theta is the vector of weights.\n",
    "# alpha is the learning rate. This will be determined by me.\n",
    "# def gradientDescent(X, y, theta, alpha):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the vectorized Gradient Descent here. Do NOT use any Loops when computing!\n",
    "# UNCOMMENT WHEN READY!\n",
    "# X is the matrix of features. Each index of x corresponds to the element in y. Ex. x[0][] -> y[0]\n",
    "# y is the vector of outputs. Each index of y corresponds to the row in x. Ex. y[0] -> x[0][]\n",
    "# theta is the vector of weights.\n",
    "# alpha is the learning rate. This will be determined by me.\n",
    "# def vectorized_gradientDescent(X, y, theta, alpha):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

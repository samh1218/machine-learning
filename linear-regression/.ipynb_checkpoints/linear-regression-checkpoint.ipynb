{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some Terms to Review :\n",
    "1. $x_j^{i}$ = the 'i' represents what training example this feature is from and the 'j' represents what feature it is in the feature set of that training example.\n",
    "2. We use capitalized letters for variables to denote a MATRIX only. Vectors and scalars are written lowercase letters.\n",
    "3. 'scalar' is a single-value output. Vector is a single-column matrix.\n",
    "    1. scalar example : y = 1\n",
    "    2. vector example : $x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ ... \\\\ x_n \\end{bmatrix}$\n",
    "    3. matrix example : $K = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$\n",
    "\n",
    "## What is Regression?\n",
    "\n",
    "'Regression' is a study of correlations between two variables. What you are essentially doing in a regression task is to find the nature of the relationship between two things, and if somehow the two are correlated. \n",
    "\n",
    "So, what is 'linear regression?' We know from grade school that the goal of linear regression is to come up with the 'best fit line.'\n",
    "\n",
    "#### Review Questions : \n",
    "Q1 : What does the 'best fit line' represent?\n",
    "Q2 : How do we compute the 'best fit line?'\n",
    "Q3 : What is the point of 'linear regression'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Univariate Linear Regression\n",
    "\n",
    "In Grade School, we learned that a Univariate Linear Regression is written as :\n",
    "\n",
    "$y = mx + b$ where :\n",
    "1. $y = $ the output\n",
    "2. $m$ and $b$ are 'weights'\n",
    "3. $x$ is the feature\n",
    "\n",
    "In essence, we are trying to predict the output by manipulating the values of 'm' and 'x.'\n",
    "\n",
    "While this function is accurate, it's not very useful for us when we need to include more weights and features to do better prediction of y.\n",
    "\n",
    "#### In Machine Learning, we write the same function as :\n",
    "\n",
    "$h(x)=\\theta_1 * x_1 + \\theta_2 * x_2$ where :\n",
    "1. $\\theta_1$ and $\\theta_2$ are 'weights'\n",
    "2. $x_1$ is the default feature and equal to $1$\n",
    "3. $x_2$ is the single feature for the univariate linear regression.\n",
    "\n",
    "Although the same function, we like to keep the weights and features to have the same variable name so that we can more easily identify what is what.\n",
    "\n",
    "#### We can re-write this same function into a vectorized format\n",
    "$h_\\theta(x) = \\theta_1 * x_1 + \\theta_2 * x_2$\n",
    "\n",
    "$h_\\theta(X) = \\theta^{T}x$ where :\n",
    "1. $\\theta^{T}$ is a transpose vector : $\\begin{bmatrix}\\theta_1 & \\theta_2 \\end{bmatrix}$\n",
    "2. $x$ is a vector for features : $\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions :\n",
    "\n",
    "$Q_1$ : During the training phase, which of the following are we trying to tweak?\n",
    "\n",
    "a.) the weights, b.) the features, c.) the output\n",
    "\n",
    "$Q_2$ : What is the difference between 'dimensions' and 'features'? (This is a trick question)\n",
    "\n",
    "$Q_3$ : What is 'Regression'?\n",
    "\n",
    "$Q_4$ : In a regression line, with each increase or decrease of the standard deviation in feature x, how much is the output $y$ increased? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some crucial libraries for building our Univariate Linear Regression Model\n",
    "import numpy as np                # for some crucial (linear algebra) computation\n",
    "import pandas as pd               # for data structures and data analysis\n",
    "import matplotlib.pyplot as plt  # for plotting our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN READY!\n",
    "# Here, you should implement the hypothesis of a SINGLE TRAINING EXAMPLE.\n",
    "#\n",
    "#@param Theta : a vector of weights (numpy)\n",
    "#@param x : a vector of features for ith training example (numpy)\n",
    "#@return a scalar, floating-point value, rounded to the second decimal point.\n",
    "#        This represents the predicted price of 'avg cost for two' for this training example.\n",
    "#\n",
    "#def h(Theta, x):\n",
    "#\n",
    "#\n",
    "#\n",
    "#TESTS (NOT USED FOR GRADING)----------\n",
    "#x1 = np.array([1, 121.0275, 14.56544])\n",
    "#Theta = np.array([25, 0.5, 0.25])\n",
    "#y1 = 89.16\n",
    "#assert(h(Theta,x1) == y1)\n",
    "#x2 = np.array([1, 121.0141, 14.55371])\n",
    "#y2 = 89.15\n",
    "#assert(h(Theta,x2) == y2)\n",
    "#x3 = np.array([1, 121.057, 14.23768])\n",
    "#y3 = 89.09\n",
    "#assert(h(Theta,x3) == y3)\n",
    "#x4 = np.array([1, -47.8818, -15.7641])\n",
    "#y4 = -2.88\n",
    "#assert(h(Theta,x4) == y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest for the General/Multivariate Linear Regression Function for a Single Training Example\n",
    "\n",
    "Now, let's to implement a linear regression function when we have 2 features, excluding the default feature of 1.\n",
    "\n",
    "## Hypothesis for a Single Training EXAMPLE :\n",
    "A vectorized linear regression is :\n",
    "\n",
    "$h_\\theta(x) = \\theta^{T}x$ where $x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ ... \\\\ x_n \\end{bmatrix}$ and $\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ ... \\\\ \\theta_n \\end{bmatrix}$ and 'n' is the number of features.\n",
    "\n",
    "As you can see, all you really need to do is add in more features in the feature vector and an equal number of weights in the weight vector. The function itself does not change.\n",
    "\n",
    "So what does this mean? This exact function you see above is what you will use for all forms of linear regression.\n",
    "\n",
    "NOTE : The output of the above function is for ONE expected value. In other words, the above function is only good for ONE TRAINING EXAMPLE (NOT training set).\n",
    "\n",
    "## Hypothesis for a Training Set\n",
    "A training set is made up of training examples. Each training set is made up of :\n",
    "1. a set of x's (vector)\n",
    "2. and a single output 'y' (scalar)\n",
    "\n",
    "As mentioned, the above equation solves for a single training example.\n",
    "\n",
    "What we want is a hypothesis that can solve an entire training set.\n",
    "\n",
    "### Task : Vectorized Multivariate Linear Regression\n",
    "\n",
    "Given a training set, write a general multivariate linear regression hypothesis. Your general multivariate linear regression should receive all the training examples and weights, and output a vector of scalar values, each pertaining to a training example.\n",
    "\n",
    "Remember, each training set is made up of each training example.\n",
    "\n",
    "DO NOT USE :\n",
    "1. For Loops or Recursions\n",
    "2. Single Training Example Hypothesis Function\n",
    "\n",
    "Each Training example is made up of :\n",
    "1. $x^{i}$ : a vector of features for the ith training example\n",
    "2. $\\theta$ : a vector of weights\n",
    "3. $y^{i}$ : a scalar output.\n",
    "\n",
    "Pro Tips :\n",
    "1. Use Matrix for the features to store the vectors of features for every training example.\n",
    "2. Your 'Y' should be a vector of outputs.\n",
    "3. The vector of weights should remain as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param Theta : the weights. This is a numpy array\n",
    "# @param X : the features. This is a numpy matrix.\n",
    "# @return a vector of scalar values, each position corresponding to \n",
    "# the index of the training example.\n",
    "#-------------------------\n",
    "# UNCOMMENT WHEN READY\n",
    "#-------------------------\n",
    "#def genH(Theta, X):\n",
    "\n",
    "# TESTS (Not used for grading)---------------\n",
    "# Construct your variables down below\n",
    "#gX = np.matrix('1 121.0275 14.56544; 1 121.0141 14.55371; 1 -47.8818 -15.7641')\n",
    "#gTheta = np.array([25, 0.5, 0.25])\n",
    "#gY = np.array([89.16, 89.15, -2.88])\n",
    "#assert(np.array_equal(genH(gTheta, gX), gY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training : The Quest for the Ideal 'Weights'\n",
    "As mentioned in the past, the goal of 'training' in machine learning is to find the values for the weights of the function so that the function with those weights and features can predict an outcome it has never seen quite accurately.\n",
    "\n",
    "In Training, we use the 'cost function' to measure how well the predictor line fits to the data we are seeing (known as 'observations' or the 'training set examples').\n",
    "\n",
    "### The Cost Function\n",
    "Essentially, the cost of the regression line is the vertical distance between each observation (e.g. training example) to the regression line.\n",
    "\n",
    "The regression line with the least amount of error is the one we call the 'best fit' line or the final regression line at the end of training.\n",
    "\n",
    "#### So What is the Cost Function?\n",
    "As we said, the distance between each point to the line is itself the 'cost' of the regression line.\n",
    "\n",
    "So, we need :\n",
    "\n",
    "- Summation to compute the total distance by adding up every point's distance to the regression line\n",
    "- For each value in the summation, an equation to compute the 'distance'\n",
    "\n",
    "Let's see the function for a univariate linear regression line.\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = (\\sum_{i=1}^{M}(h_{\\theta_0,\\theta_1}(x^{i}) - y^{i})^{2}) \\div (2m) $\n",
    "\n",
    "NOTE :\n",
    "1. The exponent for 'X' and 'y' are indices for identifying in the training set. A training set is composed of training examples and each training example has a set of X's associated to a scalar 'y.'\n",
    "2. In this example, $x^{i}$ is a vector and $y^{i}$ is a scalar.\n",
    "\n",
    "Let's now try to convert this so we have vector inputs and not scalar inputs.\n",
    "\n",
    "We keep the '2' to help us later with the partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Cost Function\n",
    "\n",
    "Now, try to come up with first the regular cost function, then the vectorized cost function.\n",
    "\n",
    "The regular cost function should look similar to the above cost function.\n",
    "\n",
    "Your parameters should be :\n",
    "1. $\\theta$ : Vector of Weights. These are always the same for every training example.\n",
    "2. $x^{i}$ : vector of features for the ith training example\n",
    "3. $y$ : vector for expected outputs\n",
    "\n",
    "The cost function should return the total cost of the line.\n",
    "\n",
    "USE :\n",
    "1. the cost_one_example function inside the 'cost' function.\n",
    "2. For loops to iterate through both the 'listX' and 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Regular Cost Function Here.\n",
    "#\n",
    "# NOTE : the size of Y will give you the # of training examples in your training set\n",
    "#\n",
    "#@param Theta the array of weights\n",
    "#@param x the array of features for ith training example\n",
    "#@param y the expected output for ith training example\n",
    "#@return expected cost for one training example.\n",
    "#def cost_one_example(Theta, x, y):\n",
    "#\n",
    "#\n",
    "#\n",
    "#@param Theta the array of weights\n",
    "#@param listX : list of vectors of X (a matrix)\n",
    "#@param Y \n",
    "#def cost(Theta, listX, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Cost Function\n",
    "Now, let's try to come up with the vectorized cost function. Like the vectorized hypothesis, the vectorized cost function should utilize the following parameters :\n",
    "\n",
    "1. $\\theta$ : vector of weights\n",
    "2. $X$ : matrix of weights. Each row represents a single training example's features.\n",
    "3. $y$ : vector of outputs.\n",
    "\n",
    "The vectorized cost function SHOULD NOT use any for loops. DO NOT use the cost_one_example function above to implement the vectorized cost function. Rely only on matrix multiplication, addition, subtraction, and transposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the vectorized cost function here : \n",
    "# UNCOMMENT WHEN READY!\n",
    "#@param listX the matrix of features\n",
    "#@param Theta the array of weights\n",
    "#@param Y vector of expected outputs\n",
    "#@return the cost of the regression line\n",
    "#def vCost(Theta, listX, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
